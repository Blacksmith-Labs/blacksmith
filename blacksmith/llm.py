import openai
import json
from typing import Optional
from blacksmith.config.constants import ChatRoles
from blacksmith.config.prompts import DEFAULT_OBSERVATION
from blacksmith.config.environment import MODEL, TEMPERATURE
from blacksmith.utils.registry import registry
from blacksmith.tools import use_tool
from pydantic import BaseModel


class ChatModelMessage(BaseModel):
    role: str
    content: str


class FunctionCall(BaseModel):
    """
    A class representing a function call generated from a LLM.
    """

    tool: str | None
    args: str | None

    def execute(self, verbose=False):
        """
        Execute the function call generated by the language model and return the resulting value.
        """
        try:
            tool_result = use_tool(tool_name=self.tool, args=json.loads(self.args))
            if verbose:
                print("Result of function call:", tool_result)
            return FunctionCallResult(tool=self.tool, args=self.args, result=tool_result)
        except Exception as e:
            print(f"Error executing {self.tool}: {e}")


class FunctionCallResult(FunctionCall):
    """
    A class representing the result of calling a LLM function.
    """

    result: str = None

    def generate_observation(self) -> ChatModelMessage:
        """
        Generates a ChatModelMessage as an observation of the result of tool usage.
        """
        return ChatModelMessage(
            role=ChatRoles.USER,
            content=DEFAULT_OBSERVATION.format(result=self.result, tool=self.tool, args=self.args),
        )


class LLMResponse(BaseModel):
    content: str | None
    function_call: Optional[FunctionCall] = None


def llm_call(messages=[], tools=None, verbose=False) -> LLMResponse:
    # OpenAI
    tools = tools or registry.get_tools()
    res = openai.ChatCompletion.create(
        model=MODEL,
        messages=messages,
        temperature=TEMPERATURE,
        functions=tools,
    )["choices"][0]["message"]

    res = res.to_dict()
    if verbose:
        print(res)

    fc = res.get("function_call").to_dict()
    return LLMResponse(
        content=res.get("content"),
        function_call=FunctionCall(tool=fc.get("name"), args=fc.get("arguments")),
    )
