import openai
import json
import os
from typing import Optional, List, Any
from blacksmith.config.constants import ChatRoles
from blacksmith.config.prompts import DEFAULT_OBSERVATION
from blacksmith.config.constants import TYPE_MAPPINGS
from blacksmith.context import Config
from blacksmith.utils.registry import registry
from blacksmith.tools import use_tool
from pydantic import BaseModel


# Code from https://github.com/jxnl/instructor
def _remove_a_key(d, remove_key) -> None:
    """Remove a key from a dictionary recursively"""
    if isinstance(d, dict):
        for key in list(d.keys()):
            if key == remove_key:
                del d[key]
            else:
                _remove_a_key(d[key], remove_key)


class Schema(BaseModel):
    # Code from https://github.com/jxnl/instructor
    @classmethod
    @property
    def schema(cls):
        schema = cls.model_json_schema()
        parameters = {k: v for k, v in schema.items() if k not in ("title", "description")}
        parameters["required"] = sorted(
            k for k, v in parameters["properties"].items() if not "default" in v
        )

        _remove_a_key(parameters, "additionalProperties")
        _remove_a_key(parameters, "title")

        return {
            "name": schema["title"],
            "description": "Complete the function call with the choices given.",
            "parameters": parameters,
        }


class Choice(Schema):
    """
    A class representing a choice between multiple options. You can pass this to `generate_from` to perform classification tasks.

    Inherits from `Schema`.

    Attributes:
        options (List[Any]): The options to choose from.

    Usage:
    ```
    cities = Choice(options=["San Francisco", "Los Angeles", "New York City"])
    generate_from(cities, "The Golden Gate Bridge")
    "San Francisco"
    ```
    """

    options: List[Any]

    # We over-ride the `schema()` method from the parent class
    # Unfortunately, this needs to be an instance method as the option type is not known until object instantiation
    def schema(cls):
        model = cls.model_dump()
        option_type = TYPE_MAPPINGS[type(model["options"][0]).__name__]
        return {
            "name": "Choice",
            "description": "Complete the function call with the choices given.",
            "parameters": {
                "type": "object",
                "properties": {
                    "choice": {"type": option_type, "enum": [v for v in model["options"]]}
                },
            },
        }


def generate_from(obj: Schema, query: str) -> dict | str:
    """
    Generates from a given `Schema` object. This can be used to perform classification tasks, or to generate JSON corresponding to a specific schema.

    Args:
        obj (Schema): The class to generate to.
        query (str): The query to generate from.

    Returns:
        dict | str: The response generated by the LLM. This is a dictionary if you pass in a `Schema`, otherwise a string.

    Usage:
    ```
    cities = Choice(options=["San Francisco", "Los Angeles", "New York City"])
    generate_from(cities, "The Golden Gate Bridge")
    "San Francisco"
    ```
    """
    is_choice = isinstance(obj, Choice)
    object_schema = obj.schema() if is_choice else obj.schema

    c = Conversation(
        system_prompt="You are a helpful assistant who only has access to a single function."
    )
    resp = c.ask(query, functions=[object_schema], function_call={"name": object_schema["name"]})
    response = json.loads(resp.function_call.args)
    return response if not is_choice else response["choice"]


class ChatMessage(BaseModel):
    """
    A class representing a chat message in a `Conversation`.

    Attributes:
        role (`ChatRoles`): The author of the message (system, assistant, or user.)
        content (str): The content of the message.

    Usage:
    ```
    c = Conversation()
    message = ChatMessage(role=ChatRoles.USER, content="Hey, how are you?")
    c.add_message(message)
    ```
    """

    role: ChatRoles
    content: str

    class Config:
        use_enum_values = True


class FunctionCall(BaseModel):
    """
    A class representing a function call generated from a LLM call using `Conversation.ask`.

    Attributes:
        tool (str | None): The name of the tool to call.
        args (dict | None): A dictionary of arguments to pass to the tool.

    Methods:
        inspect(): Prints the JSON representation of FunctionCall to stdout.
        execute(): Executes the function call generated by the language model and returns the resulting value.

    Usage:
    ```
        resp = Conversation().ask("What is 5 * 6")
        function_call = resp.function_call
        function_call.execute()
    ```
    """

    tool: str | None
    args: dict | None

    def inspect(self):
        """
        Prints the JSON representation of FunctionCall to stdout.
        """
        json = self.model_dump_json(indent=4)
        print(json)

    def execute(self, debug: bool = False):
        """
        Execute the function call generated by the language model and return the resulting value.
        """
        try:
            tool_result = use_tool(tool_name=self.tool, args=self.args)
            if debug:
                print(f"Calling {self.tool} with {self.args}", flush=True)
                print("Result of function call:", tool_result, flush=True)
            return FunctionCallResult(tool=self.tool, args=self.args, result=tool_result)
        except Exception as e:
            print(f"Error executing {self.tool}: {e}", flush=True)


class FunctionCallResult(FunctionCall):
    """
    A class representing the result of calling a LLM function.

    Inherits from `FunctionCall`.

    Attributes:
        result (Any): The result of the function call.

    Methods:
        generate_observation(observation_prompt: str) -> ChatMessage: Generates a ChatMessage as an observation of the result of tool usage.

    Usage:
    ```
    resp = c.ask("What is 5 * 10")
    if resp.has_function_call():
        result = resp.execute_function_call()
        result.inspect()
        "50"
    ```
    """

    result: Any = None

    def generate_observation(self, observation_prompt: str = DEFAULT_OBSERVATION) -> ChatMessage:
        """
        Generates a ChatMessage as an observation of the result of tool usage.
        You can override the default observation prompt by passing in an observation_prompt parameter.
        """
        return ChatMessage(
            role=ChatRoles.USER,
            content=observation_prompt.format(result=self.result, tool=self.tool, args=self.args),
        )


class LLMResponse(BaseModel):
    """
    Class representing a response from a completion.
    Returned from `Conversation.ask`.

    Attributes:
        content (str | None): The content of the response. `None` if there is a pending function call.
        function_call (Optional[FunctionCall]): The function call object, if the model wants to perform a function call. Defaults to `None`.

    Methods:
        execute_function_call() -> `FunctionCallResult`:
            Executes a function call.
        has_function_call() -> bool:
            Returns true if the `LLMResponse` object contains a function call.

    Usage:
    ```
    resp = c.ask("What is 5 * 6")
    if resp.has_function_call():
        result = resp.execute_function_call()
    ```
    """

    content: str | None
    function_call: Optional[FunctionCall] = None

    def execute_function_call(self, debug: bool = False) -> FunctionCallResult:
        """
        Executes a function call.
        """
        if not self.function_call:
            return ValueError("Function call not found.")
        return self.function_call.execute(debug=debug)

    def has_function_call(self) -> bool:
        """
        Returns true if the `LLMResponse` object contains a function call.
        """
        return self.function_call is not None


class Conversation(BaseModel):
    """
    Class representing a conversation with a language model.

    Attributes:
        system_prompt (Optional[str]): The system prompt to use for the conversation. Defaults to an empty string.
        messages (Optional[List[ChatMessage]]): A list of `ChatMessage` objects representing the conversation history. Defaults to an empty list.
        config (Optional[Config]): A `Config` object containing the configuration settings for the language model. Defaults to `None`.

    Methods:
        ask() -> `LLMResponse`:
            Sends a prompt plus the current message chain to the language model.
            You can change the role by passing in a ChatRoles parameter (defaults to User).
            Returns a LLMResponse object.

    Usage:
    ```
        conversation = Conversation()
        response = conversation.ask("Hello, how are you?")
    ```
    """

    system_prompt: Optional[str] = ""
    messages: Optional[List[ChatMessage]] = []
    config: Optional[Config] = None

    def ask(
        self,
        prompt: str,
        functions: list[dict] = [],
        function_call: str | dict = "auto",
        use_functions: bool = True,
        role: ChatRoles = ChatRoles.USER,
        debug: bool = False,
    ) -> LLMResponse:
        """
        Sends a prompt plus the current message chain to the language model.
        You can change the role by passing in a ChatRoles parameter (defaults to User).
        Returns a LLMResponse object.
        """

        # init config
        # this cannot be defaulted since we need `model_post_init` to be called after the first instantiation
        if not self.config:
            self.config = Config()
            self.config.load()

        # init system message
        if self.system_prompt and not self.messages:
            self.messages = [ChatMessage(role=ChatRoles.SYSTEM, content=self.system_prompt)]

        # default to all available functions
        if len(functions) == 0:
            functions = registry.get_tools()

        self.add_message(ChatMessage(role=role, content=prompt))

        return self._send(
            functions=functions if use_functions else [], function_call=function_call, debug=debug
        )

    def _send(
        self, functions: list[dict], function_call: str | dict = "auto", debug=False
    ) -> LLMResponse:
        try:
            openai.api_key = self.config.api_key

            if not functions:
                res = openai.ChatCompletion.create(
                    model=self.config.model,
                    messages=[message.model_dump() for message in self.messages],
                    temperature=self.config.temperature,
                )["choices"][0]["message"]
                return LLMResponse(content=res.to_dict().get("content"))

            res = openai.ChatCompletion.create(
                model=self.config.model,
                messages=[message.model_dump() for message in self.messages],
                temperature=self.config.temperature,
                functions=functions,
                function_call=function_call,
            )["choices"][0]["message"]

            res = res.to_dict()
            if debug:
                print(res)
            fc = res.get("function_call")
            if fc:
                fc = fc.to_dict()

            return LLMResponse(
                content=res.get("content"),
                function_call=FunctionCall(
                    tool=fc.get("name"), args=json.loads(fc.get("arguments"))
                )
                if fc
                else None,
            )
        except Exception as e:
            print(f"Error sending {self.messages}: {e}")

    def add_message(self, message: ChatMessage) -> None:
        """
        Adds a `ChatMessage` to the message chain.
        """
        self.messages.append(message)

    def continue_from_result(self, fcr: FunctionCallResult, stop: bool = False):
        """
        Generates an observation from a function call result and sends another request to the LLM.

        Args:
            fcr (FunctionCallResult): A `FunctionCallResult` object representing the result of a function call.
            stop (bool, optional): A boolean indicating whether to stop the conversation after this request. Defaults to `False`.

        Returns:
            LLMResponse: A `LLMResponse` object representing the response from the language model.

        Usage:
        ```
            result = resp.execute_function_call()
            final_answer = conversation.continue_from_result(result, stop=True)
        ```
        """
        observation = fcr.generate_observation()
        self.add_message(observation)
        functions = [] if stop else registry.get_tools()
        return self._send(functions=functions)

    def with_config(self, config: Config):
        """
        Sets the configuration for a `Conversation`.
        """
        self.config = config
